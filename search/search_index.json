{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Dagster Platform Documentation","text":"\ud83d\ude80 This is a comprehensive data engineering platform built with Dagster.    Get started or explore the platform architecture."},{"location":"#overview","title":"Overview","text":"<p>Welcome to the Dagster Platform Documentation. This platform provides a modern, scalable foundation for building reliable, cost-effective data pipelines while maintaining strict governance and security standards for UK Civil Service data engineering teams.</p> <ul> <li> <p> Getting Started</p> <p>Quick start guide to get your first pipeline running</p> <p> Start here</p> </li> <li> <p>:material-architecture: Architecture</p> <p>Learn about the platform's design and core components</p> <p> Explore architecture</p> </li> <li> <p> Platform Core</p> <p>Deep dive into resources, asset checks, and core utilities</p> <p> Core components</p> </li> <li> <p> Observability</p> <p>Monitoring, alerting, and operational excellence</p> <p> Monitoring guide</p> </li> </ul>"},{"location":"#key-features","title":"Key Features","text":""},{"location":"#asset-centric-architecture","title":"\ud83c\udfd7\ufe0f Asset-Centric Architecture","text":"<p>Built around Dagster assets with bronze, silver, and gold data layers for better lineage tracking and governance.</p>"},{"location":"#intelligent-compute-orchestration","title":"\u26a1 Intelligent Compute Orchestration","text":"<p>Automatic selection of compute resources based on workload characteristics: - Lambda for stateless validation and triggers - Fargate for short-lived containerized tasks - EKS + Karpenter for parallel processing with auto-scaling - EC2 Spot/Batch for long-running or GPU-intensive workloads</p>"},{"location":"#comprehensive-data-quality","title":"\ud83d\udd0d Comprehensive Data Quality","text":"<ul> <li>Automated data quality validation with asset checks</li> <li>Schema validation and monitoring</li> <li>Data freshness and continuity checks</li> <li>Built-in PII detection and masking</li> </ul>"},{"location":"#cost-management","title":"\ud83d\udcb0 Cost Management","text":"<ul> <li>Spot instance integration for cost-effective processing</li> <li>Per-job cost attribution and tracking</li> <li>Automated resource right-sizing</li> <li>Idle resource detection</li> </ul>"},{"location":"#security-governance","title":"\ud83d\udd12 Security &amp; Governance","text":"<ul> <li>Security by design principles</li> <li>GDPR and UK Data Protection compliance</li> <li>Built-in governance policies</li> <li>Comprehensive audit trails</li> </ul>"},{"location":"#platform-components","title":"Platform Components","text":""},{"location":"#core-infrastructure","title":"Core Infrastructure","text":"Platform CoreProject TemplateObservability <p>The <code>dagster-platform</code> package provides shared utilities and resources:</p> <ul> <li>Resources: AWS, Database, and API integrations</li> <li>Asset Checks: Data quality and validation framework</li> <li>Partitions: Common partitioning strategies</li> <li>IO Managers: Standardized data I/O patterns</li> <li>Compute Kinds: Compute type definitions</li> </ul> <p>The <code>dagster-project-template</code> provides:</p> <ul> <li>Cookiecutter Template: Rapid project setup</li> <li>Bronze Assets: Raw data ingestion patterns</li> <li>Silver Assets: Data cleaning and validation</li> <li>Gold Assets: Business logic and analytics</li> <li>Job Definitions: Pipeline orchestration</li> </ul> <p>Comprehensive monitoring and alerting:</p> <ul> <li>Sensors: File-based and schedule-based triggers</li> <li>Hooks: Execution lifecycle management</li> <li>Metrics: Custom metrics collection</li> <li>Alerts: Multi-channel alerting system</li> </ul>"},{"location":"#quick-start","title":"Quick Start","text":"<p>Get your first pipeline running in minutes:</p> <pre><code># Install cookiecutter\npip install cookiecutter\n\n# Generate a new project\ncookiecutter dagster-project-template/\n\n# Install dependencies\ncd my-dagster-project\npip install -e .\n\n# Start the development server\ndagster dev\n</code></pre>"},{"location":"#architecture-overview","title":"Architecture Overview","text":"<pre><code>graph TB\n    A[Data Sources] --&gt; B[Bronze Layer]\n    B --&gt; C[Silver Layer]\n    C --&gt; D[Gold Layer]\n    D --&gt; E[Data Consumers]\n\n    B --&gt; F[Lambda Compute]\n    C --&gt; G[Fargate Compute]\n    D --&gt; H[EKS Compute]\n\n    I[Platform Core] --&gt; B\n    I --&gt; C\n    I --&gt; D\n\n    J[Observability] --&gt; B\n    J --&gt; C\n    J --&gt; D\n\n    K[Governance] --&gt; B\n    K --&gt; C\n    K --&gt; D</code></pre>"},{"location":"#design-principles","title":"Design Principles","text":"<p>Core Principles</p> <ul> <li>Asset-centric architecture: All data processing built around Dagster assets</li> <li>Separation of concerns: Platform team manages infrastructure, project teams focus on business logic</li> <li>Configuration as code: All pipeline definitions managed through version-controlled YAML</li> <li>Cost-conscious compute: Intelligent workload placement across compute types</li> <li>Security by design: Built-in compliance and governance tooling</li> </ul>"},{"location":"#support-and-community","title":"Support and Community","text":"<ul> <li>Documentation: This comprehensive guide covers all aspects of the platform</li> <li>API Reference: Auto-generated API documentation from code</li> <li>Examples: Real-world examples and patterns</li> <li>Troubleshooting: Common issues and solutions</li> </ul>"},{"location":"#status","title":"Status","text":"<p>Stable Platform Core Beta Project Templates Alpha Advanced Features  </p> <p>This documentation is automatically generated from code and updated with each release. For the latest changes, see the changelog.</p>"},{"location":"architecture/platform-overview/","title":"Platform Overview","text":"<p>The Dagster Platform is designed as a modern, scalable data engineering solution that follows best practices for enterprise data processing. This document provides a comprehensive overview of the platform architecture and design decisions.</p>"},{"location":"architecture/platform-overview/#architecture-principles","title":"Architecture Principles","text":""},{"location":"architecture/platform-overview/#1-asset-centric-design","title":"1. Asset-Centric Design","text":"<p>The platform is built around Dagster assets rather than traditional jobs or tasks. This approach provides:</p> <ul> <li>Clear data lineage - Every data transformation is tracked</li> <li>Dependency management - Automatic resolution of asset dependencies</li> <li>Incremental computation - Only recompute what has changed</li> <li>Observable data - Rich metadata and quality metrics</li> </ul>"},{"location":"architecture/platform-overview/#2-medallion-architecture","title":"2. Medallion Architecture","text":"<p>Data flows through three distinct layers:</p> <pre><code>graph TB\n    subgraph \"Data Sources\"\n        A[Operational DBs]\n        B[APIs]\n        C[File Systems]\n        D[Streaming]\n    end\n\n    subgraph \"Bronze Layer\"\n        E[Raw Data Assets]\n        F[Data Ingestion]\n        G[Minimal Processing]\n    end\n\n    subgraph \"Silver Layer\"\n        H[Cleaned Data]\n        I[Data Validation]\n        J[Standardization]\n    end\n\n    subgraph \"Gold Layer\"\n        K[Business Metrics]\n        L[Aggregations]\n        M[ML Features]\n    end\n\n    subgraph \"Data Consumers\"\n        N[Dashboards]\n        O[ML Models]\n        P[Reports]\n    end\n\n    A --&gt; E\n    B --&gt; E\n    C --&gt; E\n    D --&gt; E\n\n    E --&gt; H\n    F --&gt; I\n    G --&gt; J\n\n    H --&gt; K\n    I --&gt; L\n    J --&gt; M\n\n    K --&gt; N\n    L --&gt; O\n    M --&gt; P</code></pre>"},{"location":"architecture/platform-overview/#3-intelligent-compute-orchestration","title":"3. Intelligent Compute Orchestration","text":"<p>The platform automatically selects the most appropriate compute resource based on workload characteristics:</p> Compute Type Ideal For Cost Scaling AWS Lambda Quick transformations, triggers Very Low Automatic AWS Fargate Medium workloads, containers Low Manual Amazon EKS Large workloads, parallel processing Medium Auto/Manual AWS Batch Long-running, batch processing Variable Automatic"},{"location":"architecture/platform-overview/#core-components","title":"Core Components","text":""},{"location":"architecture/platform-overview/#platform-core-dagster-platform","title":"Platform Core (<code>dagster-platform</code>)","text":"<p>The platform core provides shared utilities and abstractions:</p> <pre><code># Resource management\nfrom platform_core.resources import S3Resource, PostgresResource\n\n# Data quality framework\nfrom platform_core.asset_checks import validate_schema, check_freshness\n\n# Compute optimization\nfrom platform_core.sdk.decorators import lambda_compute, fargate_compute\n</code></pre>"},{"location":"architecture/platform-overview/#key-modules","title":"Key Modules:","text":"<p>Resources (<code>platform_core.resources</code>) - Database connections (PostgreSQL, Redshift, Snowflake) - Cloud storage (S3, GCS, Azure Blob) - API integrations (REST, GraphQL) - Secrets management</p> <p>Asset Checks (<code>platform_core.asset_checks</code>) - Schema validation - Data quality monitoring - Freshness checks - Custom business rule validation</p> <p>SDK (<code>platform_core.sdk</code>) - Asset decorators for bronze/silver/gold layers - Compute-specific decorators - Factory functions for common patterns - Type definitions and utilities</p> <p>Observability (<code>platform_core.observability</code>) - Execution hooks for monitoring - Sensor definitions for event-driven processing - Metrics collection and reporting - Alert management</p>"},{"location":"architecture/platform-overview/#project-template-dagster-project-template","title":"Project Template (<code>dagster-project-template</code>)","text":"<p>A Cookiecutter template providing:</p> <ul> <li>Standardized project structure</li> <li>Example implementations for each data layer</li> <li>Configuration templates for different environments</li> <li>CI/CD pipeline definitions</li> <li>Testing frameworks and examples</li> </ul>"},{"location":"architecture/platform-overview/#data-flow-architecture","title":"Data Flow Architecture","text":""},{"location":"architecture/platform-overview/#1-bronze-layer-raw-data-ingestion","title":"1. Bronze Layer - Raw Data Ingestion","text":"<p>Purpose: Ingest raw data with minimal processing</p> <pre><code>@bronze_asset(\n    name=\"raw_customer_data\",\n    description=\"Raw customer data from CRM system\"\n)\n@lambda_compute(timeout_seconds=300, memory_mb=512)\ndef raw_customer_data(context, database: PostgresResource):\n    \"\"\"Ingest raw customer data with minimal processing.\"\"\"\n    query = \"\"\"\n    SELECT customer_id, first_name, last_name, email, \n           created_at, updated_at\n    FROM customers\n    WHERE updated_at &gt;= CURRENT_DATE - INTERVAL '1 day'\n    \"\"\"\n    return database.execute_query(query)\n</code></pre> <p>Characteristics: - Minimal data transformation - Fast ingestion using Lambda - Preserve original data structure - Fault-tolerant with retry logic</p>"},{"location":"architecture/platform-overview/#2-silver-layer-data-cleaning-and-validation","title":"2. Silver Layer - Data Cleaning and Validation","text":"<p>Purpose: Clean, validate, and standardize data</p> <pre><code>@silver_asset(\n    name=\"cleaned_customer_data\",\n    description=\"Cleaned and validated customer data\",\n    data_quality_checks=[\"no_nulls\", \"unique_values\", \"schema_validation\"]\n)\n@fargate_compute(cpu_units=512, memory_mb=1024)\ndef cleaned_customer_data(context, raw_customer_data: pd.DataFrame):\n    \"\"\"Clean and validate customer data.\"\"\"\n    df = raw_customer_data.copy()\n\n    # Data cleaning operations\n    df = remove_duplicates(df)\n    df = standardize_formats(df)\n    df = validate_business_rules(df)\n\n    return df\n</code></pre> <p>Characteristics: - Comprehensive data quality checks - Standardized data formats - Fargate compute for consistent performance - Detailed quality metrics and reporting</p>"},{"location":"architecture/platform-overview/#3-gold-layer-business-logic-and-analytics","title":"3. Gold Layer - Business Logic and Analytics","text":"<p>Purpose: Generate business metrics and analytical insights</p> <pre><code>@gold_asset(\n    name=\"customer_lifetime_value\",\n    description=\"Customer lifetime value calculations\",\n    business_owner=\"analytics_team\"\n)\n@eks_compute(node_type=\"m5.large\", min_nodes=2, max_nodes=10)\ndef customer_lifetime_value(\n    context, \n    cleaned_customer_data: pd.DataFrame,\n    order_history: pd.DataFrame\n):\n    \"\"\"Calculate customer lifetime value metrics.\"\"\"\n    return calculate_clv(cleaned_customer_data, order_history)\n</code></pre> <p>Characteristics: - Complex business logic - EKS compute for scalability - Rich metadata and lineage - Business-focused metrics</p>"},{"location":"architecture/platform-overview/#compute-orchestration-details","title":"Compute Orchestration Details","text":""},{"location":"architecture/platform-overview/#lambda-compute","title":"Lambda Compute","text":"<pre><code>@lambda_compute(\n    timeout_seconds=900,      # Maximum 15 minutes\n    memory_mb=3008,          # Up to 3GB memory\n    environment_variables={   # Custom environment\n        \"LOG_LEVEL\": \"INFO\"\n    }\n)\ndef quick_validation(context, data: pd.DataFrame):\n    \"\"\"Fast data validation using Lambda.\"\"\"\n    return validate_data_format(data)\n</code></pre> <p>Best For: - Data validation and quality checks - Simple transformations - Event-driven processing - Cost-sensitive workloads</p>"},{"location":"architecture/platform-overview/#fargate-compute","title":"Fargate Compute","text":"<pre><code>@fargate_compute(\n    cpu_units=1024,          # 1 vCPU\n    memory_mb=2048,          # 2GB memory\n    execution_role_arn=\"arn:aws:iam::123456789012:role/TaskRole\"\n)\ndef data_processing(context, raw_data: pd.DataFrame):\n    \"\"\"Medium-scale data processing on Fargate.\"\"\"\n    return process_data(raw_data)\n</code></pre> <p>Best For: - Containerized workloads - Predictable resource requirements - Medium-duration processing - Isolated execution environments</p>"},{"location":"architecture/platform-overview/#eks-compute","title":"EKS Compute","text":"<pre><code>@eks_compute(\n    node_type=\"m5.xlarge\",\n    min_nodes=1,\n    max_nodes=20,\n    spot_instances=True\n)\ndef large_scale_analytics(context, datasets: List[pd.DataFrame]):\n    \"\"\"Large-scale analytics on EKS.\"\"\"\n    return parallel_processing(datasets)\n</code></pre> <p>Best For: - Large-scale data processing - Parallel/distributed computing - Machine learning workloads - Auto-scaling requirements</p>"},{"location":"architecture/platform-overview/#resource-management","title":"Resource Management","text":""},{"location":"architecture/platform-overview/#database-resources","title":"Database Resources","text":"<pre><code># PostgreSQL connection\npostgres = PostgresResource(\n    host=\"prod-db.example.com\",\n    port=5432,\n    database=\"analytics\",\n    username=\"dagster_user\",\n    password=EnvVar(\"POSTGRES_PASSWORD\")\n)\n\n# Query execution with automatic connection management\ndf = postgres.execute_query(\"SELECT * FROM customers\")\n</code></pre>"},{"location":"architecture/platform-overview/#cloud-storage-resources","title":"Cloud Storage Resources","text":"<pre><code># S3 integration\ns3 = S3Resource(\n    bucket_name=\"analytics-data-lake\",\n    region_name=\"eu-west-2\"\n)\n\n# File operations\ns3.upload_file(\"local_file.csv\", \"processed/data.csv\")\nfiles = s3.list_objects(prefix=\"raw/\")\n</code></pre>"},{"location":"architecture/platform-overview/#api-resources","title":"API Resources","text":"<pre><code># REST API integration\napi = HTTPResource(\n    base_url=\"https://api.example.com\",\n    auth_token=EnvVar(\"API_TOKEN\")\n)\n\n# API calls with error handling\nresponse = api.get(\"/customers\", params={\"limit\": 1000})\n</code></pre>"},{"location":"architecture/platform-overview/#data-quality-framework","title":"Data Quality Framework","text":""},{"location":"architecture/platform-overview/#built-in-quality-checks","title":"Built-in Quality Checks","text":"<pre><code>from platform_core.asset_checks import (\n    check_no_nulls,\n    check_unique_values,\n    check_row_count,\n    validate_schema\n)\n\n# Automatic quality validation\n@silver_asset(data_quality_checks=[\"no_nulls\", \"unique_values\"])\ndef validated_data(context, raw_data: pd.DataFrame):\n    return clean_data(raw_data)\n</code></pre>"},{"location":"architecture/platform-overview/#custom-quality-checks","title":"Custom Quality Checks","text":"<pre><code>@asset_check(asset=\"customer_data\")\ndef validate_email_format(context, customer_data: pd.DataFrame):\n    \"\"\"Validate email format in customer data.\"\"\"\n    email_pattern = r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$'\n    invalid_emails = ~customer_data['email'].str.match(email_pattern)\n\n    return AssetCheckResult(\n        passed=invalid_emails.sum() == 0,\n        description=f\"Found {invalid_emails.sum()} invalid email addresses\"\n    )\n</code></pre>"},{"location":"architecture/platform-overview/#observability-and-monitoring","title":"Observability and Monitoring","text":""},{"location":"architecture/platform-overview/#execution-hooks","title":"Execution Hooks","text":"<pre><code>@success_hook\ndef log_success_metrics(context):\n    \"\"\"Log success metrics for monitoring.\"\"\"\n    context.log.info(f\"Asset {context.asset_key} completed successfully\")\n\n@failure_hook\ndef alert_on_failure(context):\n    \"\"\"Send alerts on pipeline failures.\"\"\"\n    send_slack_alert(f\"Pipeline failed: {context.failure_data}\")\n</code></pre>"},{"location":"architecture/platform-overview/#sensors-and-triggers","title":"Sensors and Triggers","text":"<pre><code>@sensor(job=data_processing_job)\ndef s3_file_sensor(context):\n    \"\"\"Trigger pipeline when new files arrive in S3.\"\"\"\n    new_files = check_for_new_files()\n    if new_files:\n        return RunRequest(\n            run_key=f\"s3_files_{datetime.now().isoformat()}\",\n            run_config={\"resources\": {\"s3\": {\"config\": {\"files\": new_files}}}}\n        )\n</code></pre>"},{"location":"architecture/platform-overview/#security-and-compliance","title":"Security and Compliance","text":""},{"location":"architecture/platform-overview/#data-protection","title":"Data Protection","text":"<ul> <li>Encryption at rest - All data encrypted in storage</li> <li>Encryption in transit - TLS for all communications</li> <li>Access controls - Role-based access to resources</li> <li>Audit logging - Comprehensive execution logs</li> </ul>"},{"location":"architecture/platform-overview/#compliance-features","title":"Compliance Features","text":"<ul> <li>PII Detection - Automatic identification of sensitive data</li> <li>Data Masking - Configurable data obfuscation</li> <li>Retention Policies - Automatic data lifecycle management</li> <li>Lineage Tracking - Complete data provenance</li> </ul>"},{"location":"architecture/platform-overview/#configuration-management","title":"Configuration Management","text":"<pre><code># Environment-specific configuration\nresources = {\n    \"database\": PostgresResource(\n        host=EnvVar(\"DATABASE_HOST\"),\n        password=EnvVar(\"DATABASE_PASSWORD\")\n    ),\n    \"s3\": S3Resource(\n        bucket_name=EnvVar(\"S3_BUCKET_NAME\")\n    )\n}\n</code></pre>"},{"location":"architecture/platform-overview/#performance-optimization","title":"Performance Optimization","text":""},{"location":"architecture/platform-overview/#partitioning-strategies","title":"Partitioning Strategies","text":"<pre><code># Time-based partitioning\n@asset(partitions_def=DailyPartitionsDefinition(start_date=\"2024-01-01\"))\ndef daily_metrics(context, raw_data: pd.DataFrame):\n    \"\"\"Process daily metrics with time partitioning.\"\"\"\n    partition_date = context.asset_partition_key_for_output()\n    return process_daily_data(raw_data, partition_date)\n</code></pre>"},{"location":"architecture/platform-overview/#caching-and-memoization","title":"Caching and Memoization","text":"<pre><code># Intelligent caching\n@asset(\n    freshness_policy=FreshnessPolicy(\n        maximum_lag_minutes=60,\n        cron_schedule=\"0 * * * *\"  # Hourly refresh\n    )\n)\ndef cached_metrics(context, source_data: pd.DataFrame):\n    \"\"\"Cached metrics with freshness policy.\"\"\"\n    return expensive_calculation(source_data)\n</code></pre>"},{"location":"architecture/platform-overview/#cost-optimization","title":"Cost Optimization","text":""},{"location":"architecture/platform-overview/#automatic-cost-management","title":"Automatic Cost Management","text":"<ul> <li>Spot Instance Integration - Use spot instances for cost savings</li> <li>Right-sizing - Automatic resource allocation based on workload</li> <li>Idle Detection - Shutdown unused resources</li> <li>Cost Attribution - Track costs per job and project</li> </ul>"},{"location":"architecture/platform-overview/#cost-tracking","title":"Cost Tracking","text":"<pre><code>@cost_tracked_asset(\n    compute_kind=\"eks\",\n    estimated_cost_per_run=2.50\n)\ndef expensive_operation(context, data: pd.DataFrame):\n    \"\"\"Track costs for expensive operations.\"\"\"\n    return complex_processing(data)\n</code></pre>"},{"location":"architecture/platform-overview/#next-steps","title":"Next Steps","text":"<p>Now that you understand the platform architecture:</p> <ol> <li>Data Layers - Deep dive into bronze/silver/gold patterns</li> <li>Compute Orchestration - Learn about compute selection</li> <li>Resource Management - Understand resource configuration</li> <li>Platform Core - Explore core components</li> </ol>"},{"location":"architecture/platform-overview/#architecture-benefits","title":"Architecture Benefits","text":""},{"location":"architecture/platform-overview/#developer-experience","title":"Developer Experience","text":"<ul> <li>Rapid Development - Templates and patterns accelerate development</li> <li>Type Safety - Strong typing throughout the platform</li> <li>Testing Framework - Built-in testing utilities</li> <li>Documentation - Auto-generated API docs</li> </ul>"},{"location":"architecture/platform-overview/#operational-excellence","title":"Operational Excellence","text":"<ul> <li>Monitoring - Comprehensive observability</li> <li>Alerting - Multi-channel notification system</li> <li>Scaling - Automatic resource scaling</li> <li>Recovery - Built-in failure recovery</li> </ul>"},{"location":"architecture/platform-overview/#cost-efficiency","title":"Cost Efficiency","text":"<ul> <li>Resource Optimization - Intelligent compute selection</li> <li>Spot Instances - Cost-effective compute options</li> <li>Usage Tracking - Detailed cost attribution</li> <li>Right-sizing - Automatic resource optimization</li> </ul> <p>This architecture provides a solid foundation for building scalable, maintainable, and cost-effective data pipelines while maintaining the highest standards of data quality and security.</p>"},{"location":"getting-started/installation/","title":"Installation","text":"<p>This guide will help you set up your development environment for the Dagster Platform.</p>"},{"location":"getting-started/installation/#prerequisites","title":"Prerequisites","text":"<p>Before getting started, ensure you have the following installed:</p> <ul> <li>Python 3.9+ (Python 3.11 recommended)</li> <li>Git for version control</li> <li>Docker for containerized development</li> <li>AWS CLI configured with appropriate credentials</li> </ul>"},{"location":"getting-started/installation/#environment-setup","title":"Environment Setup","text":""},{"location":"getting-started/installation/#1-python-environment","title":"1. Python Environment","text":"<p>We recommend using a virtual environment to isolate dependencies:</p> Using venvUsing condaUsing poetry <pre><code># Create virtual environment\npython -m venv dagster-platform-env\n\n# Activate environment\nsource dagster-platform-env/bin/activate  # Linux/Mac\n# or\ndagster-platform-env\\Scripts\\activate  # Windows\n</code></pre> <pre><code># Create conda environment\nconda create -n dagster-platform python=3.11\n\n# Activate environment\nconda activate dagster-platform\n</code></pre> <pre><code># Install poetry if not already installed\ncurl -sSL https://install.python-poetry.org | python3 -\n\n# Create project with poetry\npoetry new my-dagster-project\ncd my-dagster-project\npoetry shell\n</code></pre>"},{"location":"getting-started/installation/#2-install-core-dependencies","title":"2. Install Core Dependencies","text":"<p>Install the platform core package and development tools:</p> <pre><code># Install platform core\npip install -e dagster-platform/\n\n# Install development dependencies\npip install -e \"dagster-platform/[dev]\"\n\n# Install documentation dependencies\npip install -r docs/requirements.txt\n</code></pre>"},{"location":"getting-started/installation/#3-verify-installation","title":"3. Verify Installation","text":"<p>Test that everything is installed correctly:</p> <pre><code># Check Dagster version\ndagster --version\n\n# Check platform imports\npython -c \"from platform_core.resources import S3Resource; print('Platform core installed successfully')\"\n</code></pre>"},{"location":"getting-started/installation/#development-tools","title":"Development Tools","text":""},{"location":"getting-started/installation/#ide-setup","title":"IDE Setup","text":"<p>For the best development experience, we recommend using VS Code with these extensions:</p> <ul> <li>Python - Python language support</li> <li>Dagster - Dagster-specific syntax highlighting</li> <li>Git Lens - Enhanced Git capabilities</li> <li>Docker - Container development support</li> </ul>"},{"location":"getting-started/installation/#code-quality-tools","title":"Code Quality Tools","text":"<p>The platform includes pre-configured code quality tools:</p> <pre><code># Install pre-commit hooks\npre-commit install\n\n# Run linting\nruff check .\nruff format .\n\n# Run type checking\nmypy .\n\n# Run tests\npytest\n</code></pre>"},{"location":"getting-started/installation/#aws-configuration","title":"AWS Configuration","text":""},{"location":"getting-started/installation/#1-aws-credentials","title":"1. AWS Credentials","text":"<p>Configure your AWS credentials for local development:</p> <pre><code># Configure AWS CLI\naws configure\n\n# Or export environment variables\nexport AWS_ACCESS_KEY_ID=your_access_key\nexport AWS_SECRET_ACCESS_KEY=your_secret_key\nexport AWS_DEFAULT_REGION=eu-west-2\n</code></pre>"},{"location":"getting-started/installation/#2-required-aws-services","title":"2. Required AWS Services","text":"<p>Ensure you have access to the following AWS services:</p> <ul> <li>S3 - Object storage for data files</li> <li>Secrets Manager - Secure credential storage</li> <li>Lambda - Serverless compute</li> <li>ECS/Fargate - Container orchestration</li> <li>EKS - Kubernetes clusters (optional)</li> <li>Batch - Batch processing (optional)</li> </ul>"},{"location":"getting-started/installation/#docker-setup","title":"Docker Setup","text":""},{"location":"getting-started/installation/#1-docker-compose","title":"1. Docker Compose","text":"<p>The platform includes a Docker Compose configuration for local development:</p> <pre><code># Start local services\ndocker-compose up -d\n\n# Check services are running\ndocker-compose ps\n\n# View logs\ndocker-compose logs -f dagster-webserver\n</code></pre>"},{"location":"getting-started/installation/#2-local-services","title":"2. Local Services","text":"<p>The Docker setup includes:</p> <ul> <li>Dagster Webserver - UI at http://localhost:3000</li> <li>Dagster Daemon - Background job execution</li> <li>PostgreSQL - Metadata storage</li> <li>Redis - Caching and queuing</li> </ul>"},{"location":"getting-started/installation/#database-setup","title":"Database Setup","text":""},{"location":"getting-started/installation/#postgresql-local-development","title":"PostgreSQL (Local Development)","text":"<p>For local development, use the included PostgreSQL container:</p> <pre><code># Database connection details\nPOSTGRES_HOST=localhost\nPOSTGRES_PORT=5432\nPOSTGRES_DB=dagster\nPOSTGRES_USER=dagster\nPOSTGRES_PASSWORD=dagster\n</code></pre>"},{"location":"getting-started/installation/#production-database","title":"Production Database","text":"<p>For production environments, configure connection to your managed database:</p> <pre><code># dagster.yaml\nstorage:\n  postgres:\n    postgres_host: your-prod-db-host\n    postgres_port: 5432\n    postgres_db: dagster_prod\n    postgres_user: dagster_user\n    postgres_password: \n      env: POSTGRES_PASSWORD\n</code></pre>"},{"location":"getting-started/installation/#environment-variables","title":"Environment Variables","text":"<p>Create a <code>.env</code> file in your project root:</p> <pre><code># Development environment\nDAGSTER_HOME=/path/to/your/dagster/home\nDAGSTER_ENV=development\n\n# AWS Configuration\nAWS_DEFAULT_REGION=eu-west-2\nAWS_ACCESS_KEY_ID=your_access_key\nAWS_SECRET_ACCESS_KEY=your_secret_key\n\n# Database Configuration\nPOSTGRES_HOST=localhost\nPOSTGRES_PORT=5432\nPOSTGRES_DB=dagster\nPOSTGRES_USER=dagster\nPOSTGRES_PASSWORD=dagster\n\n# Platform Configuration\nPLATFORM_ENV=development\nLOG_LEVEL=INFO\n</code></pre>"},{"location":"getting-started/installation/#troubleshooting","title":"Troubleshooting","text":""},{"location":"getting-started/installation/#common-issues","title":"Common Issues","text":"Import errors for platform_core <pre><code># Ensure platform core is installed in development mode\npip install -e dagster-platform/\n\n# Check Python path\npython -c \"import sys; print(sys.path)\"\n</code></pre> AWS credential errors <pre><code># Check AWS configuration\naws sts get-caller-identity\n\n# Verify IAM permissions\naws iam get-user\n</code></pre> Docker connection issues <pre><code># Restart Docker services\ndocker-compose down\ndocker-compose up -d\n\n# Check port conflicts\nnetstat -tlnp | grep 3000\n</code></pre> Database connection errors <pre><code># Check PostgreSQL is running\ndocker-compose ps postgres\n\n# Connect to database directly\npsql -h localhost -p 5432 -U dagster -d dagster\n</code></pre>"},{"location":"getting-started/installation/#getting-help","title":"Getting Help","text":"<p>If you encounter issues:</p> <ol> <li>Check the troubleshooting guide</li> <li>Review the FAQ</li> <li>Look at the GitHub issues</li> </ol>"},{"location":"getting-started/installation/#next-steps","title":"Next Steps","text":"<p>Now that you have everything installed, let's create your first pipeline:</p> <p>Continue to Quick Start \u2192</p>"},{"location":"getting-started/installation/#development-workflow","title":"Development Workflow","text":"<p>Once installed, your typical development workflow will be:</p> <ol> <li>Activate environment: <code>source dagster-platform-env/bin/activate</code></li> <li>Start services: <code>docker-compose up -d</code></li> <li>Develop assets: Create/modify assets in your project</li> <li>Test locally: <code>dagster dev</code></li> <li>Run quality checks: <code>pre-commit run --all-files</code></li> <li>Deploy: Use CI/CD pipeline for deployment</li> </ol>"},{"location":"getting-started/installation/#optional-components","title":"Optional Components","text":""},{"location":"getting-started/installation/#jupyter-notebooks","title":"Jupyter Notebooks","text":"<p>For data exploration and prototyping:</p> <pre><code>pip install jupyter\njupyter lab\n</code></pre>"},{"location":"getting-started/installation/#data-visualization","title":"Data Visualization","text":"<p>For creating dashboards and reports:</p> <pre><code>pip install plotly streamlit\n</code></pre>"},{"location":"getting-started/installation/#mlflow","title":"MLflow","text":"<p>For machine learning model tracking:</p> <pre><code>pip install mlflow\n</code></pre> <p>These tools integrate well with the platform and can be used for advanced analytics workflows.</p>"},{"location":"getting-started/overview/","title":"Getting Started Overview","text":"<p>Welcome to the Dagster Platform! This guide will help you understand the platform architecture and get your first data pipeline running.</p>"},{"location":"getting-started/overview/#what-is-the-dagster-platform","title":"What is the Dagster Platform?","text":"<p>The Dagster Platform is a comprehensive data engineering solution built on top of Dagster, designed specifically for UK Civil Service data teams. It provides:</p> <ul> <li>Standardized patterns for data pipeline development</li> <li>Intelligent compute orchestration across AWS services</li> <li>Built-in data quality and governance features</li> <li>Cost optimization through smart resource allocation</li> <li>Security and compliance by design</li> </ul>"},{"location":"getting-started/overview/#platform-architecture","title":"Platform Architecture","text":"<p>The platform consists of two main components:</p>"},{"location":"getting-started/overview/#1-platform-core-dagster-platform","title":"1. Platform Core (<code>dagster-platform</code>)","text":"<p>A Python package containing shared utilities and infrastructure:</p> <pre><code>dagster-platform/\n\u251c\u2500\u2500 platform_core/       # Core Dagster resources and utilities\n\u251c\u2500\u2500 observability/       # Monitoring and alerting\n\u251c\u2500\u2500 sdk/                # Custom decorators and factories\n\u2514\u2500\u2500 governance/         # Compliance and security tools\n</code></pre>"},{"location":"getting-started/overview/#2-project-template-dagster-project-template","title":"2. Project Template (<code>dagster-project-template</code>)","text":"<p>A Cookiecutter template for creating new data pipeline projects:</p> <pre><code>dagster-project-template/\n\u251c\u2500\u2500 {{cookiecutter.project_name}}/\n\u2502   \u251c\u2500\u2500 assets/         # Bronze/Silver/Gold data assets\n\u2502   \u251c\u2500\u2500 jobs/           # Pipeline job definitions\n\u2502   \u251c\u2500\u2500 schedules/      # Time-based triggers\n\u2502   \u2514\u2500\u2500 sensors/        # Event-driven triggers\n\u2514\u2500\u2500 cookiecutter.json   # Template configuration\n</code></pre>"},{"location":"getting-started/overview/#data-layer-architecture","title":"Data Layer Architecture","text":"<p>The platform uses a medallion architecture with three data layers:</p>"},{"location":"getting-started/overview/#bronze-layer","title":"\ud83e\udd49 Bronze Layer","text":"<ul> <li>Purpose: Raw data ingestion</li> <li>Compute: Lambda functions</li> <li>Data: Unprocessed, exactly as received</li> <li>Example: Customer records from operational database</li> </ul>"},{"location":"getting-started/overview/#silver-layer","title":"\ud83e\udd48 Silver Layer","text":"<ul> <li>Purpose: Data cleaning and validation</li> <li>Compute: Fargate containers</li> <li>Data: Cleaned, validated, and deduplicated</li> <li>Example: Standardized customer data with quality checks</li> </ul>"},{"location":"getting-started/overview/#gold-layer","title":"\ud83e\udd47 Gold Layer","text":"<ul> <li>Purpose: Business logic and analytics</li> <li>Compute: EKS clusters</li> <li>Data: Aggregated metrics and business KPIs</li> <li>Example: Customer lifetime value calculations</li> </ul>"},{"location":"getting-started/overview/#compute-orchestration","title":"Compute Orchestration","text":"<p>The platform automatically selects the most cost-effective compute for each workload:</p> Compute Type Use Case Duration Cost Lambda Validation, triggers &lt; 15 min Lowest Fargate Data processing &lt; 4 hours Low EKS Analytics, ML Any Medium Batch Heavy processing &gt; 4 hours Variable"},{"location":"getting-started/overview/#key-features","title":"Key Features","text":""},{"location":"getting-started/overview/#data-quality-framework","title":"\ud83d\udd0d Data Quality Framework","text":"<ul> <li>Automated schema validation</li> <li>Data freshness monitoring</li> <li>Completeness and accuracy checks</li> <li>Custom business rule validation</li> </ul>"},{"location":"getting-started/overview/#observability","title":"\ud83d\udcca Observability","text":"<ul> <li>Real-time pipeline monitoring</li> <li>Automatic failure detection and recovery</li> <li>Cost tracking and optimization</li> <li>Custom metrics and alerts</li> </ul>"},{"location":"getting-started/overview/#security-compliance","title":"\ud83d\udd12 Security &amp; Compliance","text":"<ul> <li>Built-in PII detection and masking</li> <li>GDPR and UK Data Protection compliance</li> <li>Comprehensive audit logging</li> <li>Role-based access control</li> </ul>"},{"location":"getting-started/overview/#development-workflow","title":"Development Workflow","text":"<ol> <li>Generate Project: Use Cookiecutter to create new pipeline project</li> <li>Develop Assets: Create bronze, silver, and gold assets</li> <li>Add Quality Checks: Implement data validation rules</li> <li>Configure Jobs: Define pipeline execution jobs</li> <li>Deploy: Use CI/CD pipeline for automated deployment</li> </ol>"},{"location":"getting-started/overview/#next-steps","title":"Next Steps","text":"<p>Now that you understand the platform architecture, let's get started:</p> <ol> <li>Installation - Set up your development environment</li> <li>Quick Start - Create your first pipeline</li> <li>Project Setup - Configure a production-ready project</li> </ol>"},{"location":"getting-started/overview/#common-questions","title":"Common Questions","text":"Do I need to know Dagster to use this platform? <p>Basic Dagster knowledge is helpful but not required. The platform provides templates and patterns that abstract away much of the complexity. We recommend reading the Dagster documentation for deeper understanding.</p> Can I customize the compute types? <p>Yes! The platform uses decorators to specify compute types. You can easily switch between Lambda, Fargate, EKS, and Batch based on your workload requirements.</p> How do I handle sensitive data? <p>The platform includes built-in PII detection and masking capabilities. All data is encrypted at rest and in transit, and the platform follows UK government security standards.</p> What about cost optimization? <p>The platform automatically selects the most cost-effective compute for each workload and provides detailed cost tracking. It also supports spot instances and automatic resource scaling.</p>"},{"location":"getting-started/overview/#support","title":"Support","text":"<p>If you need help getting started:</p> <ul> <li>Check the troubleshooting guide</li> <li>Review the FAQ</li> <li>Look at example implementations</li> </ul> <p>Ready to start building? Let's install the platform!</p>"},{"location":"getting-started/quick-start/","title":"Quick Start","text":"<p>Get your first data pipeline running in under 10 minutes! This guide will walk you through creating a simple pipeline using the Dagster Platform.</p>"},{"location":"getting-started/quick-start/#step-1-create-a-new-project","title":"Step 1: Create a New Project","text":"<p>Use the Cookiecutter template to generate a new project:</p> <pre><code># Install cookiecutter if not already installed\npip install cookiecutter\n\n# Generate project from template\ncookiecutter dagster-project-template/\n\n# Follow the prompts:\n# project_name [my-dagster-project]: customer-analytics\n# project_description [A new Dagster data pipeline project]: Customer analytics pipeline\n# author_name [Data Engineering Team]: Your Name\n# use_dbt [yes]: no\n# compute_environment [lambda]: fargate\n# data_warehouse [postgres]: postgres\n# enable_data_quality_checks [yes]: yes\n</code></pre> <p>This creates a new project structure:</p> <pre><code>customer-analytics/\n\u251c\u2500\u2500 assets/\n\u2502   \u251c\u2500\u2500 bronze/          # Raw data ingestion\n\u2502   \u251c\u2500\u2500 silver/          # Data cleaning\n\u2502   \u2514\u2500\u2500 gold/            # Business metrics\n\u251c\u2500\u2500 jobs/                # Pipeline jobs\n\u251c\u2500\u2500 schedules/           # Time-based triggers\n\u251c\u2500\u2500 sensors/             # Event-driven triggers\n\u251c\u2500\u2500 tests/               # Test suite\n\u2514\u2500\u2500 config/              # Environment configurations\n</code></pre>"},{"location":"getting-started/quick-start/#step-2-install-project-dependencies","title":"Step 2: Install Project Dependencies","text":"<p>Navigate to your new project and install dependencies:</p> <pre><code>cd customer-analytics\npip install -e .\n</code></pre>"},{"location":"getting-started/quick-start/#step-3-configure-resources","title":"Step 3: Configure Resources","text":"<p>Edit the configuration file to set up your data connections:</p> <pre><code># config/dev.yaml\nresources:\n  database:\n    config:\n      host: localhost\n      port: 5432\n      database: analytics_dev\n      username: dagster\n      password: dagster\n\n  s3:\n    config:\n      bucket_name: my-analytics-bucket\n      region_name: eu-west-2\n</code></pre>"},{"location":"getting-started/quick-start/#step-4-explore-the-sample-pipeline","title":"Step 4: Explore the Sample Pipeline","text":"<p>The generated project includes sample assets demonstrating the bronze/silver/gold pattern:</p>"},{"location":"getting-started/quick-start/#bronze-asset-raw-data-ingestion","title":"Bronze Asset (Raw Data Ingestion)","text":"<pre><code>@bronze_asset(\n    name=\"raw_customer_data\",\n    description=\"Raw customer data from source system\"\n)\n@lambda_compute(timeout_seconds=300)\ndef raw_customer_data(context, database: PostgresResource):\n    \"\"\"Ingest raw customer data from the source database.\"\"\"\n    query = \"SELECT * FROM customers WHERE updated_at &gt;= CURRENT_DATE - INTERVAL '7 days'\"\n    return database.execute_query(query)\n</code></pre>"},{"location":"getting-started/quick-start/#silver-asset-data-cleaning","title":"Silver Asset (Data Cleaning)","text":"<pre><code>@silver_asset(\n    name=\"cleaned_customer_data\",\n    description=\"Cleaned and validated customer data\",\n    data_quality_checks=[\"no_nulls\", \"row_count\"]\n)\n@fargate_compute(cpu_units=512, memory_mb=1024)\ndef cleaned_customer_data(context, raw_customer_data: pd.DataFrame):\n    \"\"\"Clean and validate customer data.\"\"\"\n    df = raw_customer_data.copy()\n\n    # Remove duplicates\n    df = df.drop_duplicates(subset=['customer_id'])\n\n    # Clean email addresses\n    df['email'] = df['email'].str.lower().str.strip()\n\n    # Validate required fields\n    df = df.dropna(subset=['customer_id', 'first_name', 'last_name'])\n\n    return df\n</code></pre>"},{"location":"getting-started/quick-start/#gold-asset-business-metrics","title":"Gold Asset (Business Metrics)","text":"<pre><code>@gold_asset(\n    name=\"customer_lifetime_value\",\n    description=\"Customer lifetime value calculations\",\n    business_owner=\"analytics_team\"\n)\n@eks_compute(node_type=\"m5.large\")\ndef customer_lifetime_value(context, cleaned_customer_data: pd.DataFrame):\n    \"\"\"Calculate customer lifetime value.\"\"\"\n    # Business logic for CLV calculation\n    clv_df = calculate_clv(cleaned_customer_data)\n    return clv_df\n</code></pre>"},{"location":"getting-started/quick-start/#step-5-start-the-development-server","title":"Step 5: Start the Development Server","text":"<p>Launch the Dagster development server:</p> <pre><code>dagster dev\n</code></pre> <p>This starts the web interface at <code>http://localhost:3000</code> where you can:</p> <ul> <li>View your asset lineage graph</li> <li>Monitor pipeline runs</li> <li>Trigger jobs manually</li> <li>Explore asset metadata</li> </ul>"},{"location":"getting-started/quick-start/#step-6-materialize-your-first-asset","title":"Step 6: Materialize Your First Asset","text":"<p>In the Dagster UI:</p> <ol> <li>Navigate to Assets in the left sidebar</li> <li>Select the <code>raw_customer_data</code> asset</li> <li>Click Materialize to run the asset</li> <li>Monitor the execution in the Runs tab</li> </ol>"},{"location":"getting-started/quick-start/#step-7-run-the-complete-pipeline","title":"Step 7: Run the Complete Pipeline","text":"<p>Execute the full pipeline by materializing all assets:</p> <ol> <li>Go to Assets \u2192 View global asset lineage</li> <li>Select all assets (bronze \u2192 silver \u2192 gold)</li> <li>Click Materialize selected</li> <li>Watch the pipeline execute in dependency order</li> </ol>"},{"location":"getting-started/quick-start/#step-8-explore-data-quality-results","title":"Step 8: Explore Data Quality Results","text":"<p>The platform automatically runs data quality checks:</p> <ol> <li>Navigate to Asset checks in the UI</li> <li>Review the results of quality validations</li> <li>Explore any failed checks and their details</li> </ol>"},{"location":"getting-started/quick-start/#understanding-the-pipeline-flow","title":"Understanding the Pipeline Flow","text":"<pre><code>graph LR\n    A[Raw Customer Data] --&gt; B[Cleaned Customer Data]\n    B --&gt; C[Customer Lifetime Value]\n\n    A --&gt; D[Data Quality Checks]\n    B --&gt; E[Schema Validation]\n    C --&gt; F[Business Rule Validation]\n\n    style A fill:#f9f,stroke:#333,stroke-width:2px\n    style B fill:#bbf,stroke:#333,stroke-width:2px\n    style C fill:#bfb,stroke:#333,stroke-width:2px</code></pre>"},{"location":"getting-started/quick-start/#next-steps","title":"Next Steps","text":"<p>Now that you have a working pipeline, you can:</p>"},{"location":"getting-started/quick-start/#1-add-more-assets","title":"1. Add More Assets","text":"<p>Create additional assets for your specific use case:</p> <pre><code>@silver_asset(name=\"product_sales_data\")\n@fargate_compute()\ndef product_sales_data(context, raw_sales_data: pd.DataFrame):\n    \"\"\"Process sales data.\"\"\"\n    # Your data processing logic\n    return processed_data\n</code></pre>"},{"location":"getting-started/quick-start/#2-schedule-your-pipeline","title":"2. Schedule Your Pipeline","text":"<p>Add a schedule to run your pipeline automatically:</p> <pre><code>from dagster import schedule, ScheduleEvaluationContext\n\n@schedule(\n    job=full_pipeline_job,\n    cron_schedule=\"0 6 * * *\",  # Daily at 6 AM\n    description=\"Daily customer analytics pipeline\"\n)\ndef daily_pipeline_schedule(context: ScheduleEvaluationContext):\n    return {}\n</code></pre>"},{"location":"getting-started/quick-start/#3-add-custom-data-quality-checks","title":"3. Add Custom Data Quality Checks","text":"<p>Implement business-specific validation rules:</p> <pre><code>@asset_check(asset=\"customer_lifetime_value\")\ndef validate_clv_positive(context, customer_lifetime_value: pd.DataFrame):\n    \"\"\"Ensure all CLV values are positive.\"\"\"\n    negative_clv = customer_lifetime_value[customer_lifetime_value['estimated_clv'] &lt; 0]\n\n    return AssetCheckResult(\n        passed=len(negative_clv) == 0,\n        description=f\"Found {len(negative_clv)} customers with negative CLV\"\n    )\n</code></pre>"},{"location":"getting-started/quick-start/#4-set-up-alerts","title":"4. Set Up Alerts","text":"<p>Configure notifications for pipeline failures:</p> <pre><code>@failure_hook\ndef slack_failure_alert(context):\n    \"\"\"Send Slack notification on pipeline failure.\"\"\"\n    slack_webhook_url = os.getenv(\"SLACK_WEBHOOK_URL\")\n    if slack_webhook_url:\n        send_slack_message(\n            webhook_url=slack_webhook_url,\n            message=f\"Pipeline {context.job_name} failed: {context.failure_data}\"\n        )\n</code></pre>"},{"location":"getting-started/quick-start/#5-deploy-to-production","title":"5. Deploy to Production","text":"<p>Follow the deployment guide to deploy your pipeline to AWS.</p>"},{"location":"getting-started/quick-start/#common-patterns","title":"Common Patterns","text":""},{"location":"getting-started/quick-start/#working-with-different-data-sources","title":"Working with Different Data Sources","text":"DatabaseS3API <pre><code>@bronze_asset\ndef ingest_from_database(context, database: PostgresResource):\n    return database.execute_query(\"SELECT * FROM source_table\")\n</code></pre> <pre><code>@bronze_asset\ndef ingest_from_s3(context, s3: S3Resource):\n    files = s3.list_objects(prefix=\"data/\")\n    return pd.concat([pd.read_csv(f) for f in files])\n</code></pre> <pre><code>@bronze_asset\ndef ingest_from_api(context, api: HTTPResource):\n    response = api.get(\"/data/customers\")\n    return pd.DataFrame(response.json())\n</code></pre>"},{"location":"getting-started/quick-start/#data-quality-patterns","title":"Data Quality Patterns","text":"<pre><code># Schema validation\n@silver_asset(\n    schema_validated=True,\n    expected_schema={\n        \"required_columns\": [\"customer_id\", \"email\"],\n        \"column_types\": {\"customer_id\": \"integer\", \"email\": \"string\"}\n    }\n)\ndef validated_customer_data(context, raw_data: pd.DataFrame):\n    return clean_data(raw_data)\n\n# Freshness monitoring\n@gold_asset(freshness_policy_minutes=60)\ndef real_time_metrics(context, recent_data: pd.DataFrame):\n    return calculate_metrics(recent_data)\n</code></pre>"},{"location":"getting-started/quick-start/#troubleshooting","title":"Troubleshooting","text":""},{"location":"getting-started/quick-start/#common-issues","title":"Common Issues","text":"Asset won't materialize <p>Check the logs in the UI for specific error messages. Common causes: - Missing dependencies - Database connection issues - Invalid data format</p> Data quality checks failing <p>Review the check results in the UI. You may need to: - Update your data cleaning logic - Adjust quality check thresholds - Handle edge cases in your data</p> Performance issues <p>Consider: - Using appropriate compute types for your workload - Partitioning large datasets - Optimizing your data processing logic</p>"},{"location":"getting-started/quick-start/#whats-next","title":"What's Next?","text":"<p>Congratulations! You've successfully created and run your first Dagster pipeline. To continue learning:</p> <ol> <li>Project Setup - Configure a production-ready project</li> <li>Architecture Guide - Understand the platform design</li> <li>Best Practices - Learn advanced patterns</li> <li>Deployment - Deploy to production</li> </ol>"},{"location":"getting-started/quick-start/#example-projects","title":"Example Projects","text":"<p>For more complex examples, check out:</p> <ul> <li>Customer Analytics - CLV calculation and segmentation</li> <li>Product Recommendations - ML-powered recommendation engine</li> <li>Financial Reporting - Automated financial dashboards</li> <li>Real-time Monitoring - Streaming data processing</li> </ul> <p>These examples demonstrate advanced platform features and real-world usage patterns.</p>"},{"location":"reference/api-docs/","title":"API Documentation","text":"<p>This page contains API documentation for the Dagster Platform components. The full API documentation will be auto-generated from code docstrings when the platform is installed.</p>"},{"location":"reference/api-docs/#platform-core","title":"Platform Core","text":""},{"location":"reference/api-docs/#resources","title":"Resources","text":"<p>The platform provides several resource types for connecting to external systems:</p>"},{"location":"reference/api-docs/#aws-resources","title":"AWS Resources","text":"<ul> <li>S3Resource: Interact with AWS S3 buckets for file storage</li> <li>SecretsManagerResource: Manage secrets from AWS Secrets Manager</li> <li>LambdaResource: Invoke AWS Lambda functions</li> </ul>"},{"location":"reference/api-docs/#database-resources","title":"Database Resources","text":"<ul> <li>PostgresResource: Connect to PostgreSQL databases</li> <li>RedshiftResource: Connect to Amazon Redshift data warehouses</li> <li>SnowflakeResource: Connect to Snowflake data platforms</li> </ul>"},{"location":"reference/api-docs/#api-resources","title":"API Resources","text":"<ul> <li>HTTPResource: Make HTTP requests to REST APIs</li> <li>SlackResource: Send notifications to Slack channels</li> <li>EmailResource: Send email notifications via SMTP</li> </ul>"},{"location":"reference/api-docs/#asset-checks","title":"Asset Checks","text":"<p>The platform includes comprehensive data quality validation:</p>"},{"location":"reference/api-docs/#data-quality-checks","title":"Data Quality Checks","text":"<ul> <li>check_no_nulls: Verify columns have no null values</li> <li>check_unique_values: Ensure values are unique in specified columns</li> <li>check_row_count: Validate row count within expected range</li> <li>check_column_values: Verify column values are within allowed set</li> <li>check_numeric_range: Validate numeric values within range</li> <li>check_date_range: Validate date values within range</li> </ul>"},{"location":"reference/api-docs/#schema-checks","title":"Schema Checks","text":"<ul> <li>check_required_columns: Verify all required columns are present</li> <li>check_column_types: Validate column data types</li> <li>check_no_extra_columns: Ensure no unexpected columns exist</li> <li>check_column_order: Verify column order matches expectation</li> <li>validate_schema: Comprehensive schema validation</li> </ul>"},{"location":"reference/api-docs/#freshness-checks","title":"Freshness Checks","text":"<ul> <li>check_data_freshness: Verify data is within freshness window</li> <li>check_update_frequency: Validate data update frequency</li> <li>check_business_hours_data: Analyze data distribution by time</li> <li>check_data_continuity: Check for gaps in data timeline</li> </ul>"},{"location":"reference/api-docs/#sdk","title":"SDK","text":""},{"location":"reference/api-docs/#decorators","title":"Decorators","text":"<p>The SDK provides decorators for common asset patterns:</p>"},{"location":"reference/api-docs/#asset-decorators","title":"Asset Decorators","text":"<ul> <li>@bronze_asset: Decorator for raw data ingestion assets</li> <li>@silver_asset: Decorator for cleaned/validated assets</li> <li>@gold_asset: Decorator for business logic assets</li> <li>@cost_tracked_asset: Decorator for cost tracking</li> <li>@schema_validated_asset: Decorator for schema validation</li> <li>@freshness_monitored_asset: Decorator for freshness monitoring</li> </ul>"},{"location":"reference/api-docs/#compute-decorators","title":"Compute Decorators","text":"<ul> <li>@lambda_compute: Execute on AWS Lambda</li> <li>@fargate_compute: Execute on AWS Fargate</li> <li>@eks_compute: Execute on Amazon EKS</li> <li>@batch_compute: Execute on AWS Batch</li> <li>@cost_optimized_compute: Cost-optimized execution</li> </ul>"},{"location":"reference/api-docs/#factories","title":"Factories","text":"<p>Factory functions for creating common asset patterns:</p>"},{"location":"reference/api-docs/#asset-factories","title":"Asset Factories","text":"<ul> <li>create_ingestion_asset: Create data ingestion assets</li> <li>create_transformation_asset: Create data transformation assets</li> <li>create_output_asset: Create data output/export assets</li> <li>create_multi_output_asset: Create multi-output assets</li> </ul>"},{"location":"reference/api-docs/#observability","title":"Observability","text":""},{"location":"reference/api-docs/#hooks","title":"Hooks","text":"<p>Execution lifecycle hooks for monitoring:</p>"},{"location":"reference/api-docs/#execution-hooks","title":"Execution Hooks","text":"<ul> <li>log_success_hook: Log successful executions</li> <li>log_failure_hook: Log failed executions</li> <li>cost_tracking_hook: Track execution costs</li> <li>data_lineage_hook: Track data lineage</li> <li>retry_hook: Handle retry logic</li> </ul>"},{"location":"reference/api-docs/#notification-hooks","title":"Notification Hooks","text":"<ul> <li>slack_failure_hook: Send Slack notifications on failure</li> <li>slack_success_hook: Send Slack notifications on success</li> <li>email_failure_hook: Send email notifications on failure</li> <li>create_pagerduty_hook: Create PagerDuty alerts</li> </ul>"},{"location":"reference/api-docs/#sensors","title":"Sensors","text":"<p>Event-driven processing sensors:</p>"},{"location":"reference/api-docs/#file-sensors","title":"File Sensors","text":"<ul> <li>create_s3_file_sensor: Monitor S3 for new files</li> <li>create_local_file_sensor: Monitor local directory for files</li> </ul>"},{"location":"reference/api-docs/#schedule-sensors","title":"Schedule Sensors","text":"<ul> <li>create_failure_recovery_sensor: Monitor for failures and retry</li> <li>create_sla_monitoring_sensor: Monitor SLA violations</li> <li>create_dependency_sensor: Trigger on upstream job completion</li> </ul>"},{"location":"reference/api-docs/#usage-examples","title":"Usage Examples","text":""},{"location":"reference/api-docs/#basic-asset-creation","title":"Basic Asset Creation","text":"<pre><code>from platform_core.sdk.decorators import bronze_asset, silver_asset, gold_asset\nfrom platform_core.resources import PostgresResource\n\n@bronze_asset(name=\"raw_data\")\ndef ingest_raw_data(context, database: PostgresResource):\n    return database.execute_query(\"SELECT * FROM source_table\")\n\n@silver_asset(name=\"cleaned_data\", data_quality_checks=[\"no_nulls\"])\ndef clean_data(context, raw_data):\n    return raw_data.dropna()\n\n@gold_asset(name=\"business_metrics\", business_owner=\"analytics_team\")\ndef calculate_metrics(context, cleaned_data):\n    return cleaned_data.groupby(\"category\").sum()\n</code></pre>"},{"location":"reference/api-docs/#resource-configuration","title":"Resource Configuration","text":"<pre><code>from platform_core.resources import S3Resource, PostgresResource\n\nresources = {\n    \"s3\": S3Resource(\n        bucket_name=\"my-data-bucket\",\n        region_name=\"eu-west-2\"\n    ),\n    \"database\": PostgresResource(\n        host=\"localhost\",\n        port=5432,\n        database=\"analytics\",\n        username=\"dagster\",\n        password=\"password\"\n    )\n}\n</code></pre>"},{"location":"reference/api-docs/#quality-checks","title":"Quality Checks","text":"<pre><code>from platform_core.asset_checks import check_no_nulls, validate_schema\n\n@asset_check(asset=\"customer_data\")\ndef validate_customer_data(context, customer_data):\n    # Check for null values\n    null_check = check_no_nulls(customer_data, [\"customer_id\", \"email\"])\n\n    # Validate schema\n    schema = {\n        \"required_columns\": [\"customer_id\", \"email\", \"name\"],\n        \"column_types\": {\n            \"customer_id\": \"integer\",\n            \"email\": \"string\",\n            \"name\": \"string\"\n        }\n    }\n    schema_check = validate_schema(customer_data, schema)\n\n    return null_check and schema_check\n</code></pre> <p>This documentation will be automatically generated from code docstrings when the platform modules are installed. For the latest API details, refer to the source code in the <code>dagster-platform</code> package.</p>"},{"location":"reference/troubleshooting/","title":"Troubleshooting","text":"<p>This guide helps you resolve common issues when working with the Dagster Platform.</p>"},{"location":"reference/troubleshooting/#common-issues","title":"Common Issues","text":""},{"location":"reference/troubleshooting/#installation-problems","title":"Installation Problems","text":""},{"location":"reference/troubleshooting/#import-errors","title":"Import Errors","text":"ModuleNotFoundError: No module named 'platform_core' <p>Cause: The platform core package isn't installed or not in the Python path.</p> <p>Solution: <pre><code># Install platform core in development mode\npip install -e dagster-platform/\n\n# Verify installation\npython -c \"import platform_core; print('Success')\"\n</code></pre></p> ImportError: cannot import name 'X' from 'dagster' <p>Cause: Version mismatch between Dagster and the platform.</p> <p>Solution: <pre><code># Check Dagster version\npip show dagster\n\n# Update to required version\npip install dagster&gt;=1.5.0\n</code></pre></p>"},{"location":"reference/troubleshooting/#dependency-conflicts","title":"Dependency Conflicts","text":"pip install conflicts or version incompatibilities <p>Cause: Conflicting package versions in your environment.</p> <p>Solution: <pre><code># Create a fresh virtual environment\npython -m venv fresh-env\nsource fresh-env/bin/activate\n\n# Install platform dependencies\npip install -r dagster-platform/requirements.txt\n</code></pre></p>"},{"location":"reference/troubleshooting/#asset-execution-issues","title":"Asset Execution Issues","text":""},{"location":"reference/troubleshooting/#asset-wont-materialize","title":"Asset Won't Materialize","text":"Asset fails to materialize with no clear error <p>Debugging Steps:</p> <ol> <li>Check the asset definition for syntax errors</li> <li>Verify all upstream dependencies are available</li> <li>Review the Dagster logs in the UI</li> <li>Test the asset function directly in Python</li> </ol> <pre><code># Test asset function directly\nfrom your_project.assets import your_asset\n\n# Mock the context if needed\nclass MockContext:\n    def __init__(self):\n        self.log = print\n\nresult = your_asset(MockContext(), upstream_data)\n</code></pre> DagsterInvalidDefinitionError: Asset dependencies not found <p>Cause: Asset dependencies are not properly defined or imported.</p> <p>Solution: <pre><code># Ensure all dependencies are imported\nfrom .bronze_assets import raw_data\nfrom .silver_assets import cleaned_data\n\n@asset(deps=[raw_data, cleaned_data])\ndef gold_asset(context, raw_data, cleaned_data):\n    return process_data(raw_data, cleaned_data)\n</code></pre></p>"},{"location":"reference/troubleshooting/#data-quality-check-failures","title":"Data Quality Check Failures","text":"Asset check failures causing pipeline to stop <p>Investigation:</p> <ol> <li>Review the specific check that failed</li> <li>Examine the data that caused the failure</li> <li>Determine if it's a data issue or check configuration</li> </ol> <pre><code># Debug quality check\nfrom platform_core.asset_checks import check_no_nulls\n\n# Test check manually\nresult = check_no_nulls(your_dataframe, ['column1', 'column2'])\nprint(result.description)\nprint(result.metadata)\n</code></pre>"},{"location":"reference/troubleshooting/#resource-connection-issues","title":"Resource Connection Issues","text":""},{"location":"reference/troubleshooting/#database-connection-problems","title":"Database Connection Problems","text":"Database connection timeout or authentication failure <p>Common Causes: - Incorrect connection parameters - Network connectivity issues - Authentication problems - Database server down</p> <p>Debugging: <pre><code># Test database connection directly\nimport psycopg2\n\ntry:\n    conn = psycopg2.connect(\n        host=\"your-host\",\n        port=5432,\n        database=\"your-db\",\n        user=\"your-user\",\n        password=\"your-password\"\n    )\n    print(\"Connection successful\")\nexcept Exception as e:\n    print(f\"Connection failed: {e}\")\n</code></pre></p> SSL/TLS connection issues <p>Solution: <pre><code># Configure SSL in resource\ndatabase_resource = PostgresResource(\n    host=\"your-host\",\n    port=5432,\n    database=\"your-db\",\n    username=\"your-user\",\n    password=\"your-password\",\n    connect_args={\"sslmode\": \"require\"}\n)\n</code></pre></p>"},{"location":"reference/troubleshooting/#aws-resource-issues","title":"AWS Resource Issues","text":"AWS credentials not found or invalid <p>Solution: <pre><code># Check AWS credentials\naws sts get-caller-identity\n\n# Configure credentials\naws configure\n\n# Or use environment variables\nexport AWS_ACCESS_KEY_ID=your_access_key\nexport AWS_SECRET_ACCESS_KEY=your_secret_key\nexport AWS_DEFAULT_REGION=eu-west-2\n</code></pre></p> S3 bucket access denied <p>Cause: Insufficient IAM permissions.</p> <p>Required IAM permissions: <pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"s3:GetObject\",\n                \"s3:PutObject\",\n                \"s3:DeleteObject\",\n                \"s3:ListBucket\"\n            ],\n            \"Resource\": [\n                \"arn:aws:s3:::your-bucket/*\",\n                \"arn:aws:s3:::your-bucket\"\n            ]\n        }\n    ]\n}\n</code></pre></p>"},{"location":"reference/troubleshooting/#performance-issues","title":"Performance Issues","text":""},{"location":"reference/troubleshooting/#slow-asset-execution","title":"Slow Asset Execution","text":"Assets taking too long to execute <p>Optimization Strategies:</p> <ol> <li> <p>Check compute allocation: <pre><code># Increase compute resources\n@fargate_compute(cpu_units=1024, memory_mb=2048)\ndef slow_asset(context, data):\n    return process_large_data(data)\n</code></pre></p> </li> <li> <p>Use partitioning: <pre><code># Partition large datasets\n@asset(partitions_def=DailyPartitionsDefinition(start_date=\"2024-01-01\"))\ndef partitioned_asset(context, data):\n    partition_date = context.asset_partition_key_for_output()\n    return process_partition(data, partition_date)\n</code></pre></p> </li> <li> <p>Optimize data processing: <pre><code># Use efficient pandas operations\nimport pandas as pd\n\n# Avoid loops, use vectorized operations\ndf['new_column'] = df['col1'] * df['col2']  # Good\n# df['new_column'] = df.apply(lambda x: x['col1'] * x['col2'], axis=1)  # Slow\n</code></pre></p> </li> </ol>"},{"location":"reference/troubleshooting/#memory-issues","title":"Memory Issues","text":"Out of memory errors during execution <p>Solutions:</p> <ol> <li> <p>Increase memory allocation: <pre><code>@fargate_compute(memory_mb=4096)  # Increase memory\ndef memory_intensive_asset(context, data):\n    return process_large_dataset(data)\n</code></pre></p> </li> <li> <p>Process data in chunks: <pre><code>def process_in_chunks(df, chunk_size=10000):\n    for chunk in pd.read_csv('large_file.csv', chunksize=chunk_size):\n        yield process_chunk(chunk)\n</code></pre></p> </li> <li> <p>Use appropriate data types: <pre><code># Optimize data types\ndf['int_column'] = df['int_column'].astype('int32')\ndf['category_column'] = df['category_column'].astype('category')\n</code></pre></p> </li> </ol>"},{"location":"reference/troubleshooting/#scheduling-and-sensor-issues","title":"Scheduling and Sensor Issues","text":""},{"location":"reference/troubleshooting/#schedules-not-triggering","title":"Schedules Not Triggering","text":"Scheduled jobs not running as expected <p>Check List:</p> <ol> <li>Verify the schedule is turned on in the UI</li> <li>Check the cron expression is correct</li> <li>Ensure the Dagster daemon is running</li> <li>Review the schedule logs</li> </ol> <pre><code># Test cron expression\nfrom dagster import schedule_from_partitions\nfrom croniter import croniter\n\n# Verify next run time\ncron = croniter('0 6 * * *')  # 6 AM daily\nprint(f\"Next run: {cron.get_next()}\")\n</code></pre>"},{"location":"reference/troubleshooting/#sensor-not-detecting-events","title":"Sensor Not Detecting Events","text":"File sensor not triggering on new files <p>Debugging:</p> <ol> <li>Check sensor is turned on</li> <li>Verify file path and permissions</li> <li>Test sensor logic manually</li> </ol> <pre><code># Test sensor logic\nfrom your_project.sensors import your_file_sensor\n\n# Mock sensor context\nclass MockSensorContext:\n    def __init__(self):\n        self.cursor = None\n        self.log = print\n\nresult = your_file_sensor(MockSensorContext())\nprint(result)\n</code></pre>"},{"location":"reference/troubleshooting/#deployment-issues","title":"Deployment Issues","text":""},{"location":"reference/troubleshooting/#docker-build-failures","title":"Docker Build Failures","text":"Docker build fails with dependency errors <p>Common Solutions:</p> <pre><code># Use multi-stage builds\nFROM python:3.11-slim as builder\n\n# Install system dependencies\nRUN apt-get update &amp;&amp; apt-get install -y \\\n    gcc \\\n    g++ \\\n    &amp;&amp; rm -rf /var/lib/apt/lists/*\n\n# Install Python dependencies\nCOPY requirements.txt .\nRUN pip install --no-cache-dir -r requirements.txt\n\n# Final stage\nFROM python:3.11-slim\nCOPY --from=builder /usr/local/lib/python3.11/site-packages /usr/local/lib/python3.11/site-packages\n</code></pre>"},{"location":"reference/troubleshooting/#kubernetes-deployment-issues","title":"Kubernetes Deployment Issues","text":"Pods failing to start or crashing <p>Investigation Steps:</p> <pre><code># Check pod status\nkubectl get pods\n\n# View pod logs\nkubectl logs &lt;pod-name&gt;\n\n# Describe pod for events\nkubectl describe pod &lt;pod-name&gt;\n\n# Check resource limits\nkubectl top pod &lt;pod-name&gt;\n</code></pre>"},{"location":"reference/troubleshooting/#data-quality-issues","title":"Data Quality Issues","text":""},{"location":"reference/troubleshooting/#false-positive-quality-checks","title":"False Positive Quality Checks","text":"Quality checks failing on valid data <p>Solution:</p> <ol> <li> <p>Adjust check thresholds: <pre><code># Allow some null values\n@asset_check(asset=\"customer_data\")\ndef check_null_threshold(context, customer_data):\n    null_percentage = customer_data.isnull().sum().sum() / len(customer_data)\n    return AssetCheckResult(\n        passed=null_percentage &lt; 0.05,  # Allow 5% nulls\n        description=f\"Null percentage: {null_percentage:.2%}\"\n    )\n</code></pre></p> </li> <li> <p>Update business rules: <pre><code># More flexible validation\ndef validate_email_format(email_series):\n    # Allow empty emails but validate non-empty ones\n    non_empty_emails = email_series.dropna()\n    if len(non_empty_emails) == 0:\n        return True\n\n    pattern = r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$'\n    return non_empty_emails.str.match(pattern).all()\n</code></pre></p> </li> </ol>"},{"location":"reference/troubleshooting/#getting-help","title":"Getting Help","text":""},{"location":"reference/troubleshooting/#debug-mode","title":"Debug Mode","text":"<p>Enable debug logging for more detailed information:</p> <pre><code>import logging\nlogging.basicConfig(level=logging.DEBUG)\n\n# Or set environment variable\nexport DAGSTER_LOG_LEVEL=DEBUG\n</code></pre>"},{"location":"reference/troubleshooting/#useful-commands","title":"Useful Commands","text":"<pre><code># Check Dagster status\ndagster instance info\n\n# Validate pipeline definitions\ndagster pipeline validate\n\n# Reset asset materialization\ndagster asset wipe &lt;asset_name&gt;\n\n# Check asset lineage\ndagster asset lineage &lt;asset_name&gt;\n</code></pre>"},{"location":"reference/troubleshooting/#support-resources","title":"Support Resources","text":"<ol> <li>Platform Documentation: This comprehensive guide</li> <li>Dagster Documentation: https://docs.dagster.io/</li> <li>GitHub Issues: Report bugs and feature requests</li> <li>Stack Overflow: Community support with <code>dagster</code> tag</li> </ol>"},{"location":"reference/troubleshooting/#creating-bug-reports","title":"Creating Bug Reports","text":"<p>When reporting issues, include:</p> <ol> <li>Error message and full stack trace</li> <li>Platform version and Python version</li> <li>Minimal code example that reproduces the issue</li> <li>Environment details (OS, container, cloud provider)</li> <li>Steps to reproduce the problem</li> </ol> <p>Example bug report template:</p> <p><pre><code>## Bug Description\nBrief description of the issue\n\n## Environment\n- Platform version: X.X.X\n- Python version: 3.11\n- OS: Ubuntu 20.04\n- Deployment: Docker/Kubernetes\n\n## Reproduction Steps\n1. Step 1\n2. Step 2\n3. Step 3\n\n## Expected Behavior\nWhat should happen\n\n## Actual Behavior\nWhat actually happens\n\n## Error Message\n</code></pre> Full error message and stack trace <pre><code>## Code Example\n```python\n# Minimal code that reproduces the issue\n</code></pre> ```</p> <p>This approach helps maintainers quickly understand and resolve issues.</p>"}]}